{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrkrish82/agenticAi/blob/main/docs/ais-templates/aistudio_gemini_prompt_freeform_nofiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install langchain openai json\n",
        "!pip install tiktoken  # Add tiktoken\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "\n",
        "# Set up your OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "#--- Simulated Employee Database (employees.json) ---\n",
        "employee_data = [\n",
        "  { \"id\": 123, \"name\": \"John Doe\", \"department\": \"Engineering\", \"email\": \"john.doe@example.com\" },\n",
        "  { \"id\": 456, \"name\": \"Jane Smith\", \"department\": \"Marketing\", \"email\": \"jane.smith@example.com\" },\n",
        "  { \"id\": 789, \"name\": \"Peter Jones\", \"department\": \"Sales\", \"email\": \"peter.jones@example.com\" }\n",
        "]\n",
        "\n",
        "# Create the file 'employees.json' (in Colab this will be in the virtual instance's filesystem)\n",
        "with open(\"employees.json\", \"w\") as f:\n",
        "  json.dump(employee_data, f, indent=2)\n",
        "\n",
        "\n",
        "# --- Tool Function ---\n",
        "def get_employee_record(search_criteria_json):\n",
        "     try:\n",
        "        search_criteria = json.loads(search_criteria_json)\n",
        "     except json.JSONDecodeError:\n",
        "         return \"Invalid search criteria format\"\n",
        "\n",
        "     try:\n",
        "         with open(\"employees.json\", \"r\") as f:\n",
        "             employees = json.load(f)\n",
        "     except FileNotFoundError:\n",
        "        return \"Error: employees.json not found\"\n",
        "\n",
        "     if 'id' in search_criteria:\n",
        "         employee_id = search_criteria['id']\n",
        "         for employee in employees:\n",
        "             if employee.get('id') == employee_id:\n",
        "                 return json.dumps(employee)\n",
        "         return \"Employee with ID {} not found.\".format(employee_id)\n",
        "     elif 'name' in search_criteria:\n",
        "         employee_name = search_criteria['name']\n",
        "         for employee in employees:\n",
        "            if employee.get('name') == employee_name:\n",
        "                return json.dumps(employee)\n",
        "         return \"Employee named {} not found.\".format(employee_name)\n",
        "     else:\n",
        "         return \"Invalid search criteria. Please provide name or ID\"\n",
        "\n",
        "\n",
        "# Define the Tool\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"get_employee_record\",\n",
        "        func=get_employee_record,\n",
        "        description=\"Retrieves employee records based on name or ID. Input should be a JSON object containing criteria (e.g. {'name': 'John Doe'} or {'id': 123} )\"\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "# --- Agent Prompt ---\n",
        "\n",
        "system_prompt_template = \"\"\"\n",
        "    You are an employee record retrieval assistant.\n",
        "\n",
        "    Your role is to understand user queries and extract information that can be used to query the employee records tool.\n",
        "\n",
        "    Here are the tools you have available:\n",
        "\n",
        "    {tool_description}\n",
        "\n",
        "    When the user asks for information, determine if the request is for an employee record by extracting either a name or employee ID.\n",
        "    Then, form a JSON object with either \"name\" or \"id\" as a key and its corresponding value as the value. Pass the JSON object to the get_employee_record tool.\n",
        "    If the user provides an ID, ensure it is an integer before passing the JSON to the tool.\n",
        "    If you do not have enough information to form a json object, then state you do not have enough information.\n",
        "\n",
        "    Example: User asks \"Show me the record for John Doe\".\n",
        "    Output: {\"name\": \"John Doe\"}\n",
        "\n",
        "    Example: User asks \"Find the employee with ID 123\".\n",
        "    Output: {\"id\": 123}\n",
        "\n",
        "    Example: User asks \"Get me the record for employee Jane\"\n",
        "    Output: {\"name\": \"Jane\"}\n",
        "\n",
        "    Example: User asks \"How many employees do we have?\"\n",
        "    Output: I do not have enough information.\n",
        "\n",
        "    Only return a single JSON object or 'I do not have enough information.'.\n",
        "\"\"\"\n",
        "\n",
        "# Creating the Prompt template\n",
        "system_prompt = PromptTemplate(template=system_prompt_template,\n",
        "                                  input_variables=[\"tool_description\"])\n",
        "\n",
        "# Define the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Create an LLM chain for the agent's reasoning\n",
        "llm_chain = LLMChain(llm=llm, prompt=system_prompt)\n",
        "\n",
        "\n",
        "def extract_info(input):\n",
        "  tool_description_str = \"\"\n",
        "  for tool in tools:\n",
        "        tool_description_str += f\"- {tool.name}: {tool.description}\\n\"\n",
        "  response = llm_chain.run(tool_description=tool_description_str)\n",
        "  return response\n",
        "\n",
        "\n",
        "# --- Agent Logic ---\n",
        "\n",
        "def agent(input):\n",
        "    action_input = extract_info(input)\n",
        "    if action_input == 'I do not have enough information.':\n",
        "      return AgentFinish(return_values={'output': action_input}, log=action_input)\n",
        "    else:\n",
        "      tool_output = get_employee_record(action_input)\n",
        "      return AgentFinish(return_values={'output': tool_output}, log=tool_output)\n",
        "\n",
        "\n",
        "# Define the agent as a callable function\n",
        "def agent_executor(user_input):\n",
        "    return agent(user_input)\n",
        "\n",
        "\n",
        "# --- Run the agent ---\n",
        "user_query1 = \"Show me the record for John Doe\"\n",
        "user_query2 = \"Find the employee with ID 456\"\n",
        "user_query3 = \"What's the info for Peter Jones\"\n",
        "user_query4 = \"How many employees work here?\"\n",
        "user_query5 = \"Get me the info for employee Bob\"\n",
        "\n",
        "print(f\"User Query: {user_query1}\")\n",
        "print(f\"Agent Response: {agent_executor(user_query1).return_values['output']}\\n\")\n",
        "\n",
        "print(f\"User Query: {user_query2}\")\n",
        "print(f\"Agent Response: {agent_executor(user_query2).return_values['output']}\\n\")\n",
        "\n",
        "print(f\"User Query: {user_query3}\")\n",
        "print(f\"Agent Response: {agent_executor(user_query3).return_values['output']}\\n\")\n",
        "\n",
        "print(f\"User Query: {user_query4}\")\n",
        "print(f\"Agent Response: {agent_executor(user_query4).return_values['output']}\\n\")\n",
        "\n",
        "print(f\"User Query: {user_query5}\")\n",
        "print(f\"Agent Response: {agent_executor(user_query5).return_values['output']}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform_nofiles.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}